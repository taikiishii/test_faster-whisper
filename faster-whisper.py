import pyaudio
import numpy as np
from faster_whisper import WhisperModel
import io
import wave
import time
import ctranslate2


def apply_preemphasis(x: np.ndarray, coeff: float = 0.97) -> np.ndarray:
    if x.size == 0:
        return x
    y = np.empty_like(x)
    y[0] = x[0]
    y[1:] = x[1:] - coeff * x[:-1]
    return y


def normalize_audio(audio_np: np.ndarray, target_db: float = -20.0) -> np.ndarray:
    """éŸ³å£°ã‚’ç›®æ¨™ã®dBãƒ¬ãƒ™ãƒ«ã«æ­£è¦åŒ–"""
    rms = np.sqrt(np.mean(audio_np ** 2))
    if rms < 1e-7:
        return audio_np
    target_rms = 10 ** (target_db / 20.0)
    return audio_np * (target_rms / rms)


def calculate_frame_rms(data: bytes, apply_preemph: bool = False, preemph_coeff: float = 0.97) -> float:
    """å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ã®RMSã‚’è¨ˆç®—"""
    audio_np = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
    audio_np = audio_np - float(np.mean(audio_np))
    if apply_preemph:
        audio_np = apply_preemphasis(audio_np, coeff=preemph_coeff)
    return float(np.sqrt(np.mean(audio_np ** 2)))


def measure_rms(stream, sample_rate: int, chunk_size: int, seconds: float = 2.0, apply_preemph: bool = False, preemph_coeff: float = 0.97) -> float:
    frames = []
    for _ in range(int(sample_rate / chunk_size * seconds)):
        data = stream.read(chunk_size, exception_on_overflow=False)
        frames.append(data)
    audio_np = np.frombuffer(b"".join(frames), dtype=np.int16).astype(np.float32) / 32768.0
    audio_np = audio_np - float(np.mean(audio_np))
    if apply_preemph:
        audio_np = apply_preemphasis(audio_np, coeff=preemph_coeff)
    return float(np.sqrt(np.mean(audio_np ** 2)))

def main():
    # --- CUDAã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯ ---
    cuda_available = ctranslate2.get_cuda_device_count() > 0
    
    if cuda_available:
        device = "cuda"
        # Jetsonç”¨ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›è¨­å®š
        compute_type = "int8"  # float16 â†’ int8 ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
        print("ğŸš€ CUDAãŒåˆ©ç”¨å¯èƒ½ã§ã™ã€‚GPUã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    else:
        device = "cpu"
        compute_type = "int8"
        print("âš ï¸  CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPUã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    print(f"   ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print(f"   è¨ˆç®—ã‚¿ã‚¤ãƒ—: {compute_type}")
    print("   (Jetsonç”¨ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰)\n")
    
    # --- è¨­å®š ---
    model_size = "medium"  # Jetsonå‘ã‘ã«è»½é‡åŒ–ï¼ˆtiny, base, small, medium, largeï¼‰
    
    # === ç²¾åº¦å‘ä¸Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
    beam_size = 3         # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã®ãŸã‚3ã«è¨­å®šï¼ˆ5 â†’ 10 ã§ç²¾åº¦å‘ä¸Šã€ãƒ¡ãƒ¢ãƒªå¢—åŠ ï¼‰
    temperature = 0.0     # 0.0 = æœ€ã‚‚ç¢ºå®Ÿãªèªè­˜ã€é«˜ã„ã»ã©å¤šæ§˜ãªçµæœ
    enable_audio_norm = True  # éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’æ­£è¦åŒ–ã—ã¦SNRã‚’æ”¹å–„
    normalize_target_db = -20.0  # æ­£è¦åŒ–ã®ç›®æ¨™dB
    
    sample_rate = 16000   # Whisperæ¨å¥¨ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
    chunk_size = 1024     # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
    test_record_seconds = 3  # é–‹å§‹æ™‚ã®ãƒ†ã‚¹ãƒˆéŒ²éŸ³ç§’æ•°
    enable_preemph = True   # äº‹å‰å¼·èª¿ã§SNRã‚’å°‘ã—æ”¹å–„
    preemph_coeff = 0.97
    input_device_index = None  # å¿…è¦ãªã‚‰å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ç•ªå·ã‚’æŒ‡å®š
    gate_multiplier = 1.5   # ãƒã‚¤ã‚ºã‚²ãƒ¼ãƒˆé–¾å€¤ã®ä¹—æ•°ï¼ˆä½ã„ã»ã©æ„Ÿåº¦ãŒé«˜ã„ï¼‰
    gate_enabled = True     # ãƒã‚¤ã‚ºã‚²ãƒ¼ãƒˆã®æœ‰åŠ¹/ç„¡åŠ¹
    
    # VADï¼ˆéŸ³å£°æ´»å‹•æ¤œå‡ºï¼‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    silence_threshold_multiplier = 1.2  # ç„¡éŸ³åˆ¤å®šã®åŸºæº–ï¼ˆåŸºæº–RMS Ã— ã“ã®å€¤ã‚ˆã‚Šå°ã•ã„ = ç„¡éŸ³ï¼‰
    silence_duration_sec = 0.8  # ã“ã®ç§’æ•°é€£ç¶šã§ç„¡éŸ³ãªã‚‰èªè­˜å®Ÿè¡Œ
    max_record_duration = 30.0  # æœ€å¤§éŒ²éŸ³æ™‚é–“ï¼ˆç§’ï¼‰
    
    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    print(f"ãƒ¢ãƒ‡ãƒ« '{model_size}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    model = WhisperModel(model_size, device=device, compute_type=compute_type)
    
    # PyAudioã®åˆæœŸåŒ–
    audio = pyaudio.PyAudio()
    
    # --- ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±è¡¨ç¤º ---
    print("\n=== åˆ©ç”¨å¯èƒ½ãªéŸ³å£°å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ ===")
    default_device = audio.get_default_input_device_info()
    print(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒã‚¤ã‚¹: {default_device['name']} (index: {default_device['index']})")
    for i in range(audio.get_device_count()):
        dev_info = audio.get_device_info_by_index(i)
        if dev_info['maxInputChannels'] > 0:
            print(f"  [{i}] {dev_info['name']} (channels: {dev_info['maxInputChannels']})")
    print("=" * 40)
    
    input_kwargs = dict(format=pyaudio.paInt16,
                        channels=1,
                        rate=sample_rate,
                        input=True,
                        frames_per_buffer=chunk_size)
    if input_device_index is not None:
        input_kwargs['input_device_index'] = input_device_index
    stream = audio.open(**input_kwargs)

    # --- ç’°å¢ƒãƒã‚¤ã‚ºã‚’ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ---
    print("\nç’°å¢ƒãƒã‚¤ã‚ºæ¸¬å®šä¸­â€¦ï¼ˆ2ç§’ã€é™ã‹ã«ã—ã¦ãã ã•ã„ï¼‰")
    baseline_rms = measure_rms(stream, sample_rate, chunk_size, seconds=2.0, apply_preemph=enable_preemph, preemph_coeff=preemph_coeff)
    noise_gate_rms = max(0.005, baseline_rms * gate_multiplier)
    print(f"åŸºæº–RMS={baseline_rms:.4f} â†’ ã‚²ãƒ¼ãƒˆRMS={noise_gate_rms:.4f}")
    if not gate_enabled:
        print("(ãƒã‚¤ã‚ºã‚²ãƒ¼ãƒˆã¯ç„¡åŠ¹ã§ã™)")

    # --- ãƒ†ã‚¹ãƒˆéŒ²éŸ³ãƒ•ã‚§ãƒ¼ã‚º ---
    print(f"\nãƒ†ã‚¹ãƒˆéŒ²éŸ³ã‚’é–‹å§‹ã—ã¾ã™ã€‚{test_record_seconds}ç§’ãƒã‚¤ã‚¯ã«å‘ã‹ã£ã¦è©±ã—ã¦ãã ã•ã„...")
    test_frames = []
    test_start = time.time()
    for _ in range(int(sample_rate / chunk_size * test_record_seconds)):
        data = stream.read(chunk_size, exception_on_overflow=False)
        test_frames.append(data)
    test_elapsed = time.time() - test_start
    
    test_audio_data = b"".join(test_frames)
    test_audio_np = np.frombuffer(test_audio_data, dtype=np.int16).astype(np.float32) / 32768.0
    test_audio_np = test_audio_np - float(np.mean(test_audio_np))
    test_rms = float(np.sqrt(np.mean(test_audio_np ** 2)))
    test_level = float(np.abs(test_audio_np).max())
    
    print(f"\nâœ“ ãƒ†ã‚¹ãƒˆéŒ²éŸ³å®Œäº† ({test_elapsed:.2f}ç§’) | max={test_level:.3f}, rms={test_rms:.3f}")
    
    # ãƒ†ã‚¹ãƒˆå†ç”Ÿ
    print("â–¶ ãƒ†ã‚¹ãƒˆéŒ²éŸ³ã‚’å†ç”Ÿã—ã¾ã™...")
    try:
        play_stream = audio.open(format=pyaudio.paInt16,
                                 channels=1,
                                 rate=sample_rate,
                                 output=True)
        play_stream.write(test_audio_data)
        play_stream.stop_stream()
        play_stream.close()
        print("âœ“ ãƒ†ã‚¹ãƒˆå†ç”Ÿå®Œäº†")
    except Exception as e:
        print(f"âœ— ãƒ†ã‚¹ãƒˆå†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèª
    print("\nå•é¡ŒãŒãªã‘ã‚Œã° Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„...")
    input()
    
    print("\n>>> éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ï¼ (Ctrl+Cã§çµ‚äº†)")
    print("â€» ç„¡éŸ³æ™‚ã¯å¾…æ©Ÿã€éŸ³å£°æ¤œå‡ºå¾Œã«ç„¡éŸ³ã«ãªã£ãŸã‚‰èªè­˜ã‚’å®Ÿè¡Œã—ã¾ã™\n")

    try:
        loop_count = 0
        while True:
            loop_count += 1
            frames = []
            consecutive_silence_frames = 0
            silence_duration_frames = int(silence_duration_sec * sample_rate / chunk_size)
            max_frames = int(max_record_duration * sample_rate / chunk_size)
            is_recording = False
            record_start_time = None
            
            print(f"[{loop_count}] éŸ³å£°ã‚’å¾…æ©Ÿä¸­...", end="", flush=True)
            
            # éŸ³å£°æ¤œå‡ºâ†’ç„¡éŸ³ã§è‡ªå‹•çµ‚äº†ã®ãƒ«ãƒ¼ãƒ—
            while True:
                data = stream.read(chunk_size, exception_on_overflow=False)
                frames.append(data)
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ ã®RMSã‚’è¨ˆç®—
                frame_rms = calculate_frame_rms(data, apply_preemph=enable_preemph, preemph_coeff=preemph_coeff)
                
                # éŸ³å£°æ¤œå‡ºã®åˆ¤å®š
                if not is_recording:
                    # å¾…æ©Ÿä¸­â†’éŸ³å£°æ¤œå‡ºã§é–‹å§‹
                    if frame_rms >= noise_gate_rms:
                        is_recording = True
                        record_start_time = time.time()
                        consecutive_silence_frames = 0
                        print(f"\r[{loop_count}] éŒ²éŸ³ä¸­... ", end="", flush=True)
                else:
                    # éŒ²éŸ³ä¸­
                    if frame_rms < noise_gate_rms * silence_threshold_multiplier:
                        # ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ 
                        consecutive_silence_frames += 1
                    else:
                        # éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ 
                        consecutive_silence_frames = 0
                    
                    # é€²æ—è¡¨ç¤º
                    elapsed = time.time() - record_start_time
                    print(f"\r[{loop_count}] éŒ²éŸ³ä¸­... ({elapsed:.1f}ç§’) ", end="", flush=True)
                
                # çµ‚äº†æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                if is_recording:
                    # æ¡ä»¶1: æœ€å¤§æ™‚é–“ã«åˆ°é”
                    if len(frames) >= max_frames:
                        print(f"\n  æœ€å¤§éŒ²éŸ³æ™‚é–“ã«åˆ°é”ã—ã¾ã—ãŸ")
                        break
                    # æ¡ä»¶2: ååˆ†ãªç„¡éŸ³æœŸé–“
                    if consecutive_silence_frames >= silence_duration_frames:
                        print(f"\n  ç„¡éŸ³ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
                        break
            
            # è¨˜éŒ²ã—ãŸéŸ³å£°ã‚’å‡¦ç†
            record_time = time.time() - record_start_time if record_start_time else 0
            
            # ãƒ¡ãƒ¢ãƒªä¸Šã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’WhisperãŒèª­ã‚ã‚‹å½¢å¼ã«å¤‰æ›
            audio_data = b"".join(frames)
            audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
            # DCã‚ªãƒ•ã‚»ãƒƒãƒˆé™¤å»
            audio_np = audio_np - float(np.mean(audio_np))
            # äº‹å‰å¼·èª¿ï¼ˆä»»æ„ï¼‰
            if enable_preemph:
                audio_np = apply_preemphasis(audio_np, coeff=preemph_coeff)
            
            # éŸ³å£°ãƒ¬ãƒ™ãƒ«ã®ãƒã‚§ãƒƒã‚¯
            audio_level = np.abs(audio_np).max()
            audio_rms = np.sqrt(np.mean(audio_np**2))
            
            # === ç²¾åº¦å‘ä¸Š: éŸ³å£°æ­£è¦åŒ– ===
            if enable_audio_norm:
                audio_np = normalize_audio(audio_np, target_db=normalize_target_db)
            
            clip_ratio = float(np.mean(np.abs(audio_np) >= 0.98))
            print(f"âœ“ éŒ²éŸ³å®Œäº† ({record_time:.2f}ç§’) | max={audio_level:.3f}, rms={audio_rms:.3f}, clip={clip_ratio*100:.1f}%")
            
            # éŸ³å£°ãŒå°ã•ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if audio_rms < noise_gate_rms:
                print(f"  (éŸ³å£°ãŒå°ã•ã™ãã¾ã™)")
                continue

            # ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ãŒå¤šã„å ´åˆã¯è­¦å‘Š
            if clip_ratio > 0.01:
                print("  âš ï¸ å…¥åŠ›ãŒå¤§ãã™ãã¦ã‚¯ãƒªãƒƒãƒ—ã—ã¦ã„ã¾ã™ã€‚ãƒã‚¤ã‚¯ãƒ¬ãƒ™ãƒ«ã‚’ä¸‹ã’ã¦ãã ã•ã„ã€‚")
            
            # æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ
            print("  èªè­˜ä¸­...", end="", flush=True)
            transcribe_start = time.time()
            segments, info = model.transcribe(
                audio_np, 
                language="ja", 
                beam_size=beam_size,      # ç²¾åº¦å‘ä¸Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                temperature=temperature,   # ç²¾åº¦å‘ä¸Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                vad_filter=True,  # VADãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æœ‰åŠ¹åŒ–ï¼ˆç„¡éŸ³éƒ¨åˆ†ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                vad_parameters=dict(min_silence_duration_ms=500)  # 500msä»¥ä¸Šã®ç„¡éŸ³ã§åŒºåˆ‡ã‚‹
            )
            
            segment_list = list(segments)
            transcribe_time = time.time() - transcribe_start
            
            print(f" ({transcribe_time:.2f}ç§’)")
            
            if segment_list:
                for segment in segment_list:
                    print(f"  âœ“ {segment.text}")
            else:
                print("  (èªè­˜çµæœãªã—)")

    except KeyboardInterrupt:
        print("\nçµ‚äº†ã—ã¾ã™...")
    finally:
        stream.stop_stream()
        stream.close()
        audio.terminate()

if __name__ == "__main__":
    main()
